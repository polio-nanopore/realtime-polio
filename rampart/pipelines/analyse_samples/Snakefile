import yaml 
import csv
from pytools.persistent_dict import PersistentDict

##### Configuration #####

config["basecalled_path"] = config["basecalled_path"].rstrip("/")

if config.get("annotated_path"):
    config["annotated_path"] = config["annotated_path"].rstrip("/")
else:
    config["annotated_path"] = "annotations"

if config.get("output_path"):
    config["output_path"] = config["output_path"].rstrip("/")
else:
    config["output_path"] = "analysis"

stems = PersistentDict("stem_store")


samples = {}
barcode_string = ''
barcodes = []
if config.get("samples"):
    try:
        handle = config["samples"]

        samples = yaml.safe_load(handle)

        for s in samples:
            for barcode in samples[s]:

                barcodes.append(barcode)
                barcode_string += barcode + " "
        barcode_string = barcode_string.rstrip(" ")

    except:
        handle = config["samples"]

        sample_csv = yaml.safe_load(handle)

        barcodes = sample_csv.split(',')

        barcode_string = ' '.join(barcodes)
        for barcode in barcodes:
            samples[barcode] = [barcode]

else:
    raise ValueError("Please provide a barcodes.csv or run_configuration.json")

##### Function Definitions #####

def write_csv_files_with_only_one_header(csv_files, output_file):
    write_headers = True
    with open(output_file, 'w') as fw:
        for csv_file in csv_files:
            with open(csv_file) as fr:
                for l in fr:
                    l = l.rstrip('\n')
                    if write_headers:
                        write_headers = False
                        if l.startswith("read_name"):
                            fw.write(l + '\n')
                    else:
                        fw.write(l + '\n')

##### Workflow #####

rule all:
    input:
        expand(config["output_path"] + "/consensus_sequences/{sample}.fasta",sample=samples),
        expand(config["output_path"] + "/reports/{sample}.report.md",sample=samples),
        expand(config["output_path"] + "/binned_{sample}.csv",sample=samples),
        config["output_path"] + "/sample_composition_summary.csv"

rule binlorry:
    input:
    params:
        path_to_reads = config["basecalled_path"],
        report_dir = config["annotated_path"],
        outdir = config["output_path"],
        min_read_length = config["min_read_length"],
        max_read_length = config["max_read_length"],
        barcode_str = barcode_string,
        output_prefix= config["output_path"] + "/temp/binned"
    output:
        temp(expand(config["output_path"] + "/temp/binned_{barcode}.fastq",barcode = barcodes)),
        temp(expand(config["output_path"] + "/temp/binned_{barcode}.csv",barcode = barcodes))
    shell:
        "binlorry -i {params.path_to_reads:q} "
        "-t {params.report_dir:q} "
        "-o {params.output_prefix:q} "
        "-n {params.min_read_length} "
        "-x {params.max_read_length} "
        "-v 0 "
        "--bin-by barcode "
        "--filter-by barcode {params.barcode_str} "
        "--force-output "
        "--out-report"

rule rename_to_samples:
    input:
        rules.binlorry.output
    params:
        output_prefix = config["output_path"] + "/temp/binned",
        samples = samples,
        output_path = config["output_path"]
    output:
        reads=expand(config["output_path"] + "/binned_{sample}.fastq", sample = samples),
        csv=expand(config["output_path"] + "/binned_{sample}.csv", sample = samples)
    run:
        for sample in params.samples:
            barcodes = list(params.samples[sample])
            read_str = ''
            csv_list = []
            print("Mapping {} to {}".format(barcodes, sample))

            for barcode in barcodes:
                read_file = params.output_prefix + "_" + barcode + ".fastq"
                csv_file = params.output_prefix + "_" + barcode + ".csv"

                read_str+=read_file + ' '
            
                csv_list.append(csv_file)

            output_reads = params.output_path + "/binned_" + str(sample) + ".fastq"
            output_csv = params.output_path + "/binned_" + str(sample) + ".csv"

            shell("cat " + read_str + "> " + output_reads)

            if len(csv_list) > 1:
                write_csv_files_with_only_one_header(csv_list, output_csv)
            else:
                shell("cp " + csv_list[0] + " " + output_csv)

rule assess_sample:
    input:
        reads= config["output_path"] + "/binned_{sample}.fastq",
        csv= config["output_path"] + "/binned_{sample}.csv",
        refs = workflow.current_basedir +"/../../references.fasta"
    params:
        sample = "{sample}",
        output_path = config["output_path"] + "/binned_{sample}",
        min_reads = config["min_reads"],
        min_pcent = config["min_pcent"],
        path_to_script = workflow.current_basedir
    output:
        t = temp(config["output_path"] + "/binned_{sample}/temp.txt"),
        summary = config["output_path"] + "/temp/temp_{sample}_report.txt"
    run:
        for stdout_line in shell(
            "python {params.path_to_script}/parse_ref_and_depth.py "
            "--reads {input.reads} "
            "--csv {input.csv} "
            "--output_path {params.output_path} "
            "--references {input.refs} "
            "--min_reads {params.min_reads} "
            "--min_pcent {params.min_pcent} "
            "--out_counts {output.summary} "
            "--sample {params.sample}  && touch {output.t}", iterable=True):
            print(stdout_line)
            stems.store(params.sample,stdout_line)

rule process_sample:
    input:
        rules.assess_sample.output.t,
        config=workflow.current_basedir+"/config.yaml"
    params:
        sample = "{sample}",
        output_path= config["output_path"],
        path = workflow.current_basedir
    output:
        cns = config["output_path"] + "/consensus_sequences/{sample}.fasta",
        reports = config["output_path"] + "/reports/{sample}.report.md"
    run:
        analysis_stem = stems.fetch(params.sample)
        print(params.sample, analysis_stem)

        if analysis_stem != "":
            print("Passing {} for {} into processing pipeline.".format(analysis_stem, params.sample))
            config["analysis_stem"]= analysis_stem
            shell("snakemake --nolock --snakefile {params.path}/../process_sample/Snakefile "
                        "--configfile {input.config} "
                        "--config "
                        "analysis_stem={config[analysis_stem]} "
                        "output_path={params.output_path} "
                        "sample={params.sample} "
                        "--rerun-incomplete")
        else:
            shell("touch {output.cns} && touch {output.reports}")

rule cat_sample_reports:
    input:
        expand(config["output_path"] + "/temp/temp_{sample}_report.txt", sample=samples)
    output:
        csv = config["output_path"] + "/sample_composition_summary.csv",
        temp_report = temp(config["output_path"] + "/temp/temp.csv")
    run:
        with open(output.temp_report,"w") as fw:
            fw.write("Samples,Enterovirus_D111,Coxsackievirus_A1,Coxsackievirus_A10,Coxsackievirus_A11,Coxsackievirus_A12,Coxsackievirus_A13,Coxsackievirus_A14,Coxsackievirus_A15,Coxsackievirus_A16,Coxsackievirus_A17,Coxsackievirus_A18,Coxsackievirus_A19,Coxsackievirus_A2,Coxsackievirus_A20,Coxsackievirus_A21,Coxsackievirus_A22,Coxsackievirus_A24,Coxsackievirus_A3,Coxsackievirus_A4,Coxsackievirus_A5,Coxsackievirus_A6,Coxsackievirus_A7,Coxsackievirus_A8,Coxsackievirus_A9,Coxsackievirus_B1,Coxsackievirus_B2,Coxsackievirus_B3,Coxsackievirus_B4,Coxsackievirus_B5,Coxsackievirus_B6,Echovirus_E1,Echovirus_E11,Echovirus_E12,Echovirus_E13,Echovirus_E14,Echovirus_E15,Echovirus_E16,Echovirus_E17,Echovirus_E18,Echovirus_E19,Echovirus_E2,Echovirus_E20,Echovirus_E21,Echovirus_E24,Echovirus_E25,Echovirus_E26,Echovirus_E27,Echovirus_E29,Echovirus_E3,Echovirus_E30,Echovirus_E31,Echovirus_E32,Echovirus_E33,Echovirus_E4,Echovirus_E5,Echovirus_E6,Echovirus_E7,Echovirus_E9,Enterovirus_A114,Enterovirus_A119,Enterovirus_A120,Enterovirus_A121,Enterovirus_A71,Enterovirus_A76,Enterovirus_A89,Enterovirus_A90,Enterovirus_A91,Enterovirus_A92,Enterovirus_B100,Enterovirus_B101,Enterovirus_B106,Enterovirus_B107,Enterovirus_B111,Enterovirus_B112,Enterovirus_B69,Enterovirus_B73,Enterovirus_B74,Enterovirus_B75,Enterovirus_B77,Enterovirus_B78,Enterovirus_B79,Enterovirus_B80,Enterovirus_B81,Enterovirus_B82,Enterovirus_B83,Enterovirus_B84,Enterovirus_B85,Enterovirus_B86,Enterovirus_B87,Enterovirus_B88,Enterovirus_B93,Enterovirus_B97,Enterovirus_B98,Enterovirus_C102,Enterovirus_C104,Enterovirus_C105,Enterovirus_C109,Enterovirus_C113,Enterovirus_C116,Enterovirus_C117,Enterovirus_C118,Enterovirus_C96,Enterovirus_C99,Enterovirus_D68,Enterovirus_D70,Enterovirus_D94,Poliovirus_Sabinlike1,Poliovirus_Sabinlike2,Poliovirus_Sabinlike3,Poliovirus_wt1,Poliovirus_wt2,Poliovirus_wt3,Unmapped,AmbiguousMapping\n")
        file_string = ' '.join(input)
        shell("cat {output.temp_report} " + file_string + " > {output.csv}")
        for i in input:
            shell("rm " + i)

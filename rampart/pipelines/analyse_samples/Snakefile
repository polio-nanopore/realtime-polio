import yaml 
import csv
from pytools.persistent_dict import PersistentDict

##### Configuration #####

config["basecalled_path"] = config["basecalled_path"].rstrip("/")

if config.get("annotated_path"):
    config["annotated_path"] = config["annotated_path"].rstrip("/")
else:
    config["annotated_path"] = "annotations"

if config.get("output_path"):
    config["output_path"] = config["output_path"].rstrip("/")
else:
    config["output_path"] = "analysis"

stems = PersistentDict("stem_store")

# def prepend_barcode_stem(barcode):
#     print(barcode)
#     if type(barcode) == int: 
#         return "BC"+str(barcode)
#     elif barcode.startswith("NB") or barcode.startswith("BC"):
#         return barcode


samples = {}
barcode_string = ''
barcodes = []
if config.get("samples"):
    try:
        handle = config["samples"]

        samples = yaml.safe_load(handle)

        for s in samples:
            for barcode in samples[s]:
                # checked_barcode = prepend_barcode_stem(barcode)
                barcodes.append(barcode)
                barcode_string += barcode + " "
        barcode_string = barcode_string.rstrip(" ")

    except:
        handle = config["samples"]

        sample_csv = yaml.safe_load(handle)

        barcodes = sample_csv.split(',')
        # checked_barcodes = [prepend_barcode_stem(barcode) for barcode in barcodes]
        barcode_string = ' '.join(barcodes)
        for barcode in checked_barcodes:
            samples[barcode] = [barcode]

else:
    raise ValueError("Please provide a barcodes.csv or run_configuration.json")

##### Function Definitions #####

def write_csv_files_with_only_one_header(csv_files, output_file):
    write_headers = True
    with open(output_file, 'w') as fw:
        for csv_file in csv_files:
            with open(csv_file) as fr:
                for l in fr:
                    l = l.rstrip('\n')
                    if write_headers:
                        write_headers = False
                        if l.startswith("read_name"):
                            fw.write(l + '\n')
                    else:
                        fw.write(l + '\n')

##### Workflow #####

rule all:
    input:
        expand(config["output_path"] + "/consensus_sequences/{sample}.fasta",sample=samples),
        expand(config["output_path"] + "/reports/{sample}.report.md",sample=samples),
        expand(config["output_path"] + "/binned_{sample}.csv",sample=samples),
        config["output_path"] + "/sample_composition_summary.csv"

rule binlorry:
    input:
    params:
        path_to_reads = config["basecalled_path"],
        report_dir = config["annotated_path"],
        outdir = config["output_path"],
        min_read_length = config["min_read_length"],
        max_read_length = config["max_read_length"],
        barcode_str = barcode_string,
        output_prefix= config["output_path"] + "/temp/binned"
    output:
        temp(expand(config["output_path"] + "/temp/binned_{barcode}.fastq",barcode = barcodes)),
        temp(expand(config["output_path"] + "/temp/binned_{barcode}.csv",barcode = barcodes))
    shell:
        "binlorry -i {params.path_to_reads:q} "
        "-t {params.report_dir:q} "
        "-o {params.output_prefix:q} "
        "-n {params.min_read_length} "
        "-x {params.max_read_length} "
        "-v 0 "
        "--bin-by barcode "
        "--filter-by barcode {params.barcode_str} "
        "--force-output "
        "--out-report"

rule rename_to_samples:
    input:
        rules.binlorry.output
    params:
        output_prefix = config["output_path"] + "/temp/binned",
        samples = samples,
        output_path = config["output_path"]
    output:
        reads=expand(config["output_path"] + "/binned_{sample}.fastq", sample = samples),
        csv=expand(config["output_path"] + "/binned_{sample}.csv", sample = samples)
    run:
        for sample in params.samples:
            barcodes = list(params.samples[sample])
            read_str = ''
            csv_list = []
            print("Mapping {} to {}".format(barcodes, sample))

            for barcode in barcodes:
                read_file = params.output_prefix + "_" + barcode + ".fastq"
                csv_file = params.output_prefix + "_" + barcode + ".csv"

                read_str+=read_file + ' '
            
                csv_list.append(csv_file)

            output_reads = params.output_path + "/binned_" + str(sample) + ".fastq"
            output_csv = params.output_path + "/binned_" + str(sample) + ".csv"

            shell("cat " + read_str + "> " + output_reads)

            if len(csv_list) > 1:
                write_csv_files_with_only_one_header(csv_list, output_csv)
            else:
                shell("cp " + csv_list[0] + " " + output_csv)

rule assess_sample:
    input:
        reads= config["output_path"] + "/binned_{sample}.fastq",
        csv= config["output_path"] + "/binned_{sample}.csv",
        refs = workflow.current_basedir +"/../../references.fasta"
    params:
        sample = "{sample}",
        output_path = config["output_path"] + "/binned_{sample}",
        min_reads = config["min_reads"],
        min_pcent = config["min_pcent"],
        path_to_script = workflow.current_basedir
    output:
        t = temp(config["output_path"] + "/binned_{sample}/temp.txt"),
        summary = config["output_path"] + "/temp/temp_{sample}_report.txt"
    run:
        for stdout_line in shell(
            "python {params.path_to_script}/parse_ref_and_depth.py "
            "--reads {input.reads} "
            "--csv {input.csv} "
            "--output_path {params.output_path} "
            "--references {input.refs} "
            "--min_reads {params.min_reads} "
            "--min_pcent {params.min_pcent} "
            "--out_counts {output.summary} "
            "--sample {params.sample}  && touch {output.t}", iterable=True):
            print(stdout_line)
            stems.store(params.sample,stdout_line)

rule process_sample:
    input:
        rules.assess_sample.output.t,
        config=workflow.current_basedir+"/config.yaml"
    params:
        sample = "{sample}",
        output_path= config["output_path"],
        path = workflow.current_basedir
    output:
        cns = config["output_path"] + "/consensus_sequences/{sample}.fasta",
        reports = config["output_path"] + "/reports/{sample}.report.md"
    run:
        analysis_stem = stems.fetch(params.sample)
        print(params.sample, analysis_stem)

        if analysis_stem != "":
            print("Passing {} for {} into processing pipeline.".format(analysis_stem, params.sample))
            config["analysis_stem"]= analysis_stem
            shell("snakemake --nolock --snakefile {params.path}/../process_sample/Snakefile "
                        "--configfile {input.config} "
                        "--config "
                        "analysis_stem={config[analysis_stem]} "
                        "output_path={params.output_path} "
                        "sample={params.sample} "
                        "--rerun-incomplete")
        else:
            shell("touch {output.cns} && touch {output.reports}")

rule cat_sample_reports:
    input:
        expand(config["output_path"] + "/temp/temp_{sample}_report.txt", sample=samples)
    output:
        csv = config["output_path"] + "/sample_composition_summary.csv",
        temp_report = temp(config["output_path"] + "/temp/temp.csv")
    run:
        with open(output.temp_report,"w") as fw:
            fw.write("Sample,Sabin1,Sabin2,Sabin3,Poliovirus1-wt,Poliovirus2-wt,Poliovirus3-wt,NonPolioEV,Unmapped,AmbiguousMapping\n")
        file_string = ' '.join(input)
        shell("cat {output.temp_report} " + file_string + " > {output.csv}")
        for i in input:
            shell("rm " + i)
